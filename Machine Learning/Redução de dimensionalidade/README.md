# ğŸŒ² ReduÃ§Ã£o de Dimensionalidade â€” Base de Cobertura Vegetal

Este projeto tem como objetivo aplicar tÃ©cnicas de reduÃ§Ã£o de dimensionalidade â€” **PCA**, **Kernel PCA** e **LDA** â€” utilizando uma base de dados de tipos de cobertura vegetal. O foco Ã© comparar o impacto dessas tÃ©cnicas na performance de um classificador Random Forest.

---

## ğŸ“ Estrutura do Projeto

â”œâ”€â”€ cover_type_num.pkl  
â”œâ”€â”€ dimensionalidade.ipynb  
â””â”€â”€ README.md  

---

## ğŸ“Œ Objetivos

- Carregar e preparar os dados numÃ©ricos normalizados da base de cobertura vegetal.
- Dividir os dados em treino (75%) e teste (25%), mantendo a proporÃ§Ã£o das classes.
- Aplicar reduÃ§Ã£o de dimensionalidade com:
  - PCA com 6 componentes
  - Kernel PCA com 6 componentes e kernel RBF
  - LDA com nÃºmero de componentes baseado no nÃºmero de classes
- Treinar um modelo Random Forest para cada tÃ©cnica e avaliar a acurÃ¡cia.
- Comparar o desempenho dos modelos e entender o impacto da reduÃ§Ã£o.

---

## ğŸ“Š DescriÃ§Ã£o da Base

A base contÃ©m atributos normalizados referentes a regiÃµes geogrÃ¡ficas e a variÃ¡vel alvo codificada com LabelEncoder, representando tipos de cobertura vegetal. O arquivo `cover_type_num.pkl` contÃ©m os dados prontos para uso.

---

## ğŸ” Processos realizados

### PreparaÃ§Ã£o dos dados

- Os dados foram carregados do arquivo `.pkl`.
- DivisÃ£o dos dados em treino e teste, com 25% para teste.

### PCA

- AplicaÃ§Ã£o do PCA para reduzir as dimensÃµes para 6 componentes principais.
- AvaliaÃ§Ã£o da variÃ¢ncia explicada por componente.
- Treinamento do Random Forest com dados reduzidos e avaliaÃ§Ã£o da acurÃ¡cia.

### Kernel PCA

- AplicaÃ§Ã£o do Kernel PCA com kernel RBF, mantendo 6 componentes.
- Treinamento e avaliaÃ§Ã£o semelhantes ao PCA.

### LDA

- AplicaÃ§Ã£o do LDA para reduÃ§Ã£o supervisionada, com o nÃºmero mÃ¡ximo permitido de componentes (n_classes - 1).
- Treinamento e avaliaÃ§Ã£o do modelo com os dados transformados pelo LDA.

---

## âš™ï¸ Resultados

- O PCA manteve uma boa parcela da variÃ¢ncia original e obteve uma acurÃ¡cia satisfatÃ³ria.
- O Kernel PCA, por capturar relaÃ§Ãµes nÃ£o lineares, apresentou desempenho competitivo.
- O LDA, que utiliza a informaÃ§Ã£o das classes para reduzir dimensÃµes, mostrou-se eficiente quando o nÃºmero de componentes respeitou o limite permitido.

---

## ğŸ“š Bibliotecas Utilizadas

- `pickle` para carregar os dados prÃ©-processados  
- `scikit-learn` para PCA, Kernel PCA, LDA, Random Forest, mÃ©tricas e divisÃ£o dos dados  
- `numpy` e `pandas` para manipulaÃ§Ã£o dos dados (caso necessÃ¡rio)

---

## ğŸ“ ConclusÃ£o

A reduÃ§Ã£o de dimensionalidade mostrou-se Ãºtil para simplificar os dados e acelerar o treinamento do modelo, sem perda significativa de desempenho.  
Cada tÃ©cnica possui suas vantagens: PCA para reduÃ§Ã£o geral, Kernel PCA para dados com estrutura nÃ£o linear e LDA para casos supervisionados com mÃºltiplas classes.

---

## ğŸ‘¨â€ğŸ’» Autor

Matheus â€” Estudante de AnÃ¡lise e Desenvolvimento de Sistemas com foco em CiÃªncia de Dados.
