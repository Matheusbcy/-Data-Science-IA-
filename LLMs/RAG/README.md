# üìö Projeto RAG (Retrieval-Augmented Generation) com LLM

Este projeto implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) que utiliza um Modelo de Linguagem Grande (LLM) para extrair e responder a perguntas de forma contextualizada a partir de documentos. A arquitetura RAG permite que o LLM forne√ßa respostas mais precisas e relevantes, recuperando informa√ß√µes espec√≠ficas de uma base de conhecimento antes de gerar a resposta final.

## ‚ú® Recursos

* **Processamento de Documentos:** Ingest√£o de documentos PDF e divis√£o em chunks gerenci√°veis.
* **Embeddings Vetoriais:** Cria√ß√£o de representa√ß√µes vetoriais (embeddings) dos textos para busca de similaridade.
* **Banco de Dados Vetorial:** Armazenamento eficiente dos embeddings em um banco de dados vetorial (ChromaDB).
* **Recupera√ß√£o de Contexto:** Capacidade de buscar e recuperar informa√ß√µes relevantes do banco de dados com base na pergunta do usu√°rio.
* **Gera√ß√£o Aumentada:** Utiliza√ß√£o de um LLM (ex: Llama 3) para gerar respostas informativas, enriquecidas pelo contexto recuperado.
* **Interface Simples:** Demonstra√ß√£o da invoca√ß√£o do sistema com uma pergunta direta.

--

O projeto requer um token de autentica√ß√£o do Hugging Face para acessar o modelo Llama 3.

Voc√™ pode configurar isso diretamente no seu script Python, usando `os.environ["HF_TOKEN"] = getpass("Digite seu token Hugging Face: ")`. Isso solicitar√° o token durante a execu√ß√£o.

Alternativamente, voc√™ pode criar um arquivo `.env` na raiz do projeto (requer a instala√ß√£o de `python-dotenv`) e adicionar a linha `HF_TOKEN="seu_token_aqui"` nele, substituindo "seu_token_aqui" pelo seu token real.

### 1. Preparar os Dados

Certifique-se de que o arquivo PDF que voc√™ deseja usar (`Perfil_Profissional_e_Pessoal_Matheus.pdf` no exemplo) esteja no caminho especificado: `/content/Perfil_Profissional_e_Pessoal_Matheus.pdf`. Voc√™ pode ajustar a vari√°vel `file_path` no c√≥digo para apontar para o seu documento.

### 2. Executar o Notebook/Script

O projeto √© tipicamente executado em um ambiente de notebook (Jupyter, Google Colab). Siga as c√©lulas na ordem apresentada nas suas imagens originais para realizar os seguintes passos:

1.  Carregar e dividir o documento.
2.  Gerar embeddings e armazen√°-los no vetor store.
3.  Configurar o LLM (Llama 3).
4.  Definir o template do prompt.
5.  Criar e invocar a cadeia RAG com sua pergunta.

## ‚öôÔ∏è Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal.
* **LangChain:** Framework para desenvolvimento de aplica√ß√µes com LLMs.
* **Hugging Face Transformers:** Biblioteca para modelos de linguagem.
* **Llama 3:** Modelo de Linguagem Grande (LLM) utilizado para gera√ß√£o de texto.
* **BitsAndBytes & Accelerate:** Para quantiza√ß√£o e otimiza√ß√£o de modelos.
* **Sentence-Transformers:** Para gera√ß√£o de embeddings.
* **ChromaDB:** Banco de dados vetorial para armazenamento e busca de similaridade.
* **PyPDFLoader:** Para carregamento de documentos PDF.

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Se voc√™ tiver sugest√µes, melhorias ou encontrar algum problema, sinta-se √† vontade para abrir uma issue ou enviar um pull request.
