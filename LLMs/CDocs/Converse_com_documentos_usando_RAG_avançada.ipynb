{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matheusbcy/-Data-Science-IA-/blob/main/Converse_com_documentos_usando_RAG_avan%C3%A7ada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEG8KS746Wmu"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit langchain\n",
        "!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j-hJoWdJEBmU"
      },
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf2iNG4hEGo-"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo38vjjlEJb6"
      },
      "outputs": [],
      "source": [
        "!pip install -q python-dotenv\n",
        "!npm install -q localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-groq"
      ],
      "metadata": {
        "id": "2xvnn_HuLjMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import os\n",
        "import getpass\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "ug2vrrjMmSb2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHxWcGDqEQZ0",
        "outputId": "9d3106b6-ad9e-45dc-ebc0-1c9321bbe2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P5CTavqvh74",
        "outputId": "7a7f0634-0f6a-419f-8663-49f89d32a25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting projeto3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile projeto3.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "import tempfile\n",
        "import time\n",
        "import os\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "st.set_page_config(page_title=\"Converse com documentos\", page_icon=\"ðŸ“š\")\n",
        "st.title(\"Converse com documentos ðŸ“š\")\n",
        "\n",
        "# CriaÃ§Ã£o de painel lateral na interface\n",
        "uploads = st.sidebar.file_uploader(\n",
        "    label=\"Enviar arquivos\", type=[\"pdf\"], accept_multiple_files=True\n",
        ")\n",
        "\n",
        "if not uploads:\n",
        "    st.info(\"Por favor, envie algum arquivo para continuar\")\n",
        "    st.stop()\n",
        "\n",
        "model_class = \"gemma2-9b-it\"  # moonshotai/kimi-k2-instruct\n",
        "\n",
        "def model(model=\"gemma2-9b-it\", temperature=0.6):\n",
        "    llm = ChatGroq(\n",
        "        model=model,\n",
        "        max_tokens=1024,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "# IndexaÃ§Ã£o e recuperaÃ§Ã£o\n",
        "def config_retriever(uploads):\n",
        "    # Carregamento dos documentos\n",
        "\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploads:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # DivisÃ£o em pedaÃ§os de texto\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Embedding\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    # Armazenamento\n",
        "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "    vectorstore.save_local(\"vectorstore/db_faiss\")\n",
        "\n",
        "    # ConfiguraÃ§Ã£o do Retriever\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 4}\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "\n",
        "# ConfiguraÃ§Ã£o da chain\n",
        "\n",
        "\n",
        "def config_rag_chain(model_class, retriever):\n",
        "\n",
        "    # Carregamento da LLM\n",
        "    if model_class == \"gemma2-9b-it\":\n",
        "        llm = model(model=\"gemma2-9b-it\")\n",
        "    elif model_class == \"moonshotai/kimi-k2-instruct\":\n",
        "        llm = model(model=\"moonshotai/kimi-k2-instruct\")\n",
        "\n",
        "    token_s, token_e = \"\", \"\"\n",
        "\n",
        "    # Prompt de contextualizaÃ§Ã£o\n",
        "    context_q_system_prompt = \"Give the following chat historyu and the follow-up question whitch might reference context in the chat history, formualte a standalone question which can be understood without the chat history. do NOT answer the question, just reformulate it if needed and otherwise return it as is\"\n",
        "    context_q_system_prompt = token_s + context_q_system_prompt\n",
        "    context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "    context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", context_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", context_q_user_prompt),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain para contextualizaÃ§Ã£o\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm=llm, retriever=retriever, prompt=context_q_prompt\n",
        "    )\n",
        "\n",
        "    # Prompt para perguntas e respostas\n",
        "    qa_prompt_template = \"\"\"VocÃª Ã© um assistente virtual prestativo e estÃ¡ respondendo perguntas gerais.\n",
        "    Use os seguintes pedaÃ§os de contexto recuperado para responder a perguntas.\n",
        "    Se vocÃª nÃ£o sabe a resposta, apenas diga que nÃ£o sabe. Mantenha a resposta concisa.\n",
        "    Responda em portuguÃªs.  \\n\\n\n",
        "    Pergunta: {input} \\n\n",
        "    Contexto: {context}\n",
        "    \"\"\"\n",
        "\n",
        "    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n",
        "\n",
        "    # Configurar LLM e Chain para perguntas e respostas\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "# Criando nova sessÃ£o\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content=\"OlÃ¡, sou o seu assistente virtual! Como posso ajuda vocÃª\")\n",
        "    ]\n",
        "\n",
        "if \"docs_list\" not in st.session_state:\n",
        "    st.session_state.docs_list = None\n",
        "\n",
        "if \"retriever\" not in st.session_state:\n",
        "    st.session_state.retriever = None\n",
        "\n",
        "# Recuperando messagens\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "start = time.time()\n",
        "user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n",
        "\n",
        "if user_query is not None and user_query != \"\" and uploads is not None:\n",
        "    st.session_state.chat_history.append(HumanMessage(content = user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "        if st.session_state.docs_list != uploads:\n",
        "            st.session_state.docs_list = uploads\n",
        "            st.session_state.retriever = config_retriever(uploads)\n",
        "\n",
        "        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n",
        "        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n",
        "\n",
        "        resp = result[\"answer\"]\n",
        "        st.write(resp)\n",
        "\n",
        "        # Mostrar a fonte\n",
        "        sources = result[\"context\"]\n",
        "        for idx, doc in enumerate(sources):\n",
        "            source = doc.metadata[\"source\"]\n",
        "            file = os.path.basename(source)\n",
        "            page = doc.metadata.get(\"page\", \"PÃ¡gina nÃ£o especificada\")\n",
        "\n",
        "            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n",
        "            with st.popover(ref):\n",
        "                st.caption(doc.page_content)\n",
        "\n",
        "    st.session_state.chat_history.append(AIMessage(content = resp))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Tempo: \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M54-PNNEPrxh"
      },
      "source": [
        "### ExecuÃ§Ã£o do Streamlit\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KrnuMrbds4Na"
      },
      "outputs": [],
      "source": [
        "!streamlit run projeto3.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMjgsW8c1NvZ"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
