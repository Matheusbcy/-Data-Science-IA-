{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcEOsC45vedNsOQ3Pp2sRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matheusbcy/-Data-Science-IA-/blob/main/RAG_%2B_Llama3_%2B_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFDbs2eXwyr_"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate bitsandbytes\n",
        "!pip install -q langchain langchain_community langchain-huggingface langchainhub langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import getpass\n",
        "import bs4\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder)\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.schema.runnable import RunnableLambda"
      ],
      "metadata": {
        "id": "PgsYsKkaynyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Digite seu token Hugging Face: \")"
      ],
      "metadata": {
        "id": "ZYtfndyYypGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, bnb_4bit_use_double_quant = True, bnb_4bit_quant_type = \"nf4\", bnb_4bit_compute_dtype= torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = quantization_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    task = \"text-generation\",\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 500,\n",
        "    do_sample = True,\n",
        "    repetition_penalty = 1.1,\n",
        "    return_full_text = False,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "id": "R8DY7S3Jyx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLAMA 3\n",
        "template_rag = \"\"\"Você é um assistente virtual treinado para responder perguntas com base no contexto fornecido.\n",
        "\n",
        "Contexto:\n",
        "{contexto}\n",
        "\n",
        "Pergunta:\n",
        "{pergunta}\n",
        "\n",
        "Resposta:\"\"\""
      ],
      "metadata": {
        "id": "cJTIheudyyUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregador textos (pdf, web)\n",
        "file_path = \"/content/Perfil_Profissional_e_Pessoal_Matheus.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "a2DsZK87y2ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer split do documento\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200, add_start_index = True)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "zdYM2EE9y6M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo o embedding\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "84Xubk9fy7VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Armazenamento no banco vetorial\n",
        "vectorstore = Chroma.from_documents(documents = splits, embedding = hf_embeddings)"
      ],
      "metadata": {
        "id": "-rpC4LWey9Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recuperando os textos do banco vetorial\n",
        "retriever = vectorstore.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 6})"
      ],
      "metadata": {
        "id": "ExanZsN1y-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_rag = PromptTemplate(\n",
        "    input_variables = [\"contexto\", \"pergunta\"],\n",
        "    template = template_rag\n",
        ")"
      ],
      "metadata": {
        "id": "nJQpemn7y_eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatando os textos em somente uma string\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "J1tgO9_XzAwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a pergunta para o modelo\n",
        "chain_rag = {\n",
        "    \"contexto\": retriever | RunnableLambda(format_docs),\n",
        "    \"pergunta\": RunnablePassthrough()\n",
        "} | prompt_rag | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "0XbF8AYQzFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste utilizando RAG\n",
        "\n",
        "chain_rag.invoke(\"Qual Curso Matheus está fazendo no momento?\")"
      ],
      "metadata": {
        "id": "rP-vsgDzzGMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}